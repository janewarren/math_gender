{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Reasoning Trace Analysis\n",
        "\n",
        "Analyzes the length and structure of reasoning traces from CoT/reasoning models.\n",
        "Compares reasoning effort across models, domains, and conditions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from pathlib import Path\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "sns.set_style('whitegrid')\n",
        "plt.rcParams['figure.figsize'] = (14, 6)\n",
        "plt.rcParams['figure.dpi'] = 100\n",
        "\n",
        "# ── Configuration ───────────────────────────────────────────────────────\n",
        "BASE_DIR = Path('full_results')          # Change to Path('test_output') for test results\n",
        "\n",
        "REASONING_MODELS = ['gpt-5.2', 'deepseek-v3.1', 'qwen3-235b', 'qwen3-235b-thinking', 'qwen3-next-thinking']\n",
        "NON_REASONING_MODELS = ['gpt-4o', 'qwen-coder', 'llama-4']\n",
        "ALL_MODELS = NON_REASONING_MODELS + REASONING_MODELS\n",
        "\n",
        "CONDITIONS = {\n",
        "    'regular':   {'subdir': 'results',           'suffix': ''},\n",
        "    'no_guide':  {'subdir': 'results_no_guide',  'suffix': '_no_guide'},\n",
        "    'math_only': {'subdir': 'results_math_only', 'suffix': '_math_only'},\n",
        "}\n",
        "\n",
        "print(f'Base directory: {BASE_DIR}')\n",
        "print(f'Reasoning models: {REASONING_MODELS}')\n",
        "print(f'Non-reasoning models: {NON_REASONING_MODELS}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load all results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_all_results(base_dir: Path, models: list, conditions: dict) -> pd.DataFrame:\n",
        "    \"\"\"Load all result TSVs into a single DataFrame.\"\"\"\n",
        "    frames = []\n",
        "    for condition, cfg in conditions.items():\n",
        "        results_dir = base_dir / cfg['subdir']\n",
        "        suffix = cfg['suffix']\n",
        "        for model in models:\n",
        "            model_dir = results_dir / model\n",
        "            if not model_dir.exists():\n",
        "                continue\n",
        "            for f in sorted(model_dir.glob('*_converted.tsv')):\n",
        "                try:\n",
        "                    df = pd.read_csv(f, sep='\\t')\n",
        "                except Exception as e:\n",
        "                    print(f'Error loading {f}: {e}')\n",
        "                    continue\n",
        "                # Extract domain name\n",
        "                domain = f.stem.replace('_converted', '')\n",
        "                for s in ['_no_guide', '_math_only']:\n",
        "                    domain = domain.replace(s, '')\n",
        "                df['model'] = model\n",
        "                df['condition'] = condition\n",
        "                df['domain'] = domain\n",
        "                df['is_reasoning_model'] = model in REASONING_MODELS\n",
        "                frames.append(df)\n",
        "    if not frames:\n",
        "        print('WARNING: No result files found!')\n",
        "        return pd.DataFrame()\n",
        "    data = pd.concat(frames, ignore_index=True)\n",
        "    print(f'Loaded {len(data):,} rows from {len(frames)} files')\n",
        "    return data\n",
        "\n",
        "df = load_all_results(BASE_DIR, ALL_MODELS, CONDITIONS)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Extract reasoning trace features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def extract_reasoning_features(raw: str) -> dict:\n",
        "    \"\"\"Parse a raw_response to extract reasoning trace metrics.\"\"\"\n",
        "    if pd.isna(raw):\n",
        "        return {'trace_chars': 0, 'trace_words': 0, 'has_think_tags': False,\n",
        "                'has_reasoning_tags': False, 'reasoning_text': '', 'final_answer_text': raw}\n",
        "\n",
        "    raw = str(raw)\n",
        "    total_chars = len(raw)\n",
        "    total_words = len(raw.split())\n",
        "\n",
        "    # Check for <think>...</think> tags (qwen3-235b inline reasoning)\n",
        "    think_match = re.search(r'<think>(.*?)</think>', raw, re.DOTALL)\n",
        "    # Check for [REASONING]...[/REASONING] tags (captured from reasoning_content field)\n",
        "    reasoning_match = re.search(r'\\[REASONING\\](.*?)\\[/REASONING\\]', raw, re.DOTALL)\n",
        "\n",
        "    has_think = think_match is not None\n",
        "    has_reasoning = reasoning_match is not None\n",
        "\n",
        "    if has_think:\n",
        "        reasoning_text = think_match.group(1).strip()\n",
        "        final_answer_text = raw[think_match.end():].strip()\n",
        "    elif has_reasoning:\n",
        "        reasoning_text = reasoning_match.group(1).strip()\n",
        "        final_answer_text = raw[reasoning_match.end():].strip()\n",
        "    else:\n",
        "        reasoning_text = ''\n",
        "        final_answer_text = raw.strip()\n",
        "\n",
        "    trace_chars = len(reasoning_text)\n",
        "    trace_words = len(reasoning_text.split()) if reasoning_text else 0\n",
        "\n",
        "    return {\n",
        "        'total_response_chars': total_chars,\n",
        "        'total_response_words': total_words,\n",
        "        'trace_chars': trace_chars,\n",
        "        'trace_words': trace_words,\n",
        "        'has_think_tags': has_think,\n",
        "        'has_reasoning_tags': has_reasoning,\n",
        "        'has_any_trace': has_think or has_reasoning,\n",
        "        'reasoning_text': reasoning_text,\n",
        "        'final_answer_text': final_answer_text,\n",
        "    }\n",
        "\n",
        "# Apply to all rows\n",
        "features = df['raw_response'].apply(extract_reasoning_features).apply(pd.Series)\n",
        "df = pd.concat([df, features], axis=1)\n",
        "\n",
        "# Approximate token count (rough: 1 token ≈ 4 chars for English)\n",
        "df['trace_tokens_approx'] = (df['trace_chars'] / 4).astype(int)\n",
        "df['total_tokens_approx'] = (df['total_response_chars'] / 4).astype(int)\n",
        "\n",
        "# Correct answer flag\n",
        "df['correct'] = (df['loss'] == 0).astype(int)\n",
        "\n",
        "print(f'\\nRows with reasoning traces: {df[\"has_any_trace\"].sum():,} / {len(df):,}')\n",
        "print(f'Rows with <think> tags:     {df[\"has_think_tags\"].sum():,}')\n",
        "print(f'Rows with [REASONING] tags: {df[\"has_reasoning_tags\"].sum():,}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Summary statistics: response & trace length by model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Summary table: reasoning models only\n",
        "reasoning_df = df[df['is_reasoning_model']].copy()\n",
        "\n",
        "summary = reasoning_df.groupby('model').agg(\n",
        "    n_rows=('raw_response', 'count'),\n",
        "    pct_has_trace=('has_any_trace', 'mean'),\n",
        "    avg_total_chars=('total_response_chars', 'mean'),\n",
        "    avg_trace_chars=('trace_chars', 'mean'),\n",
        "    median_trace_chars=('trace_chars', 'median'),\n",
        "    max_trace_chars=('trace_chars', 'max'),\n",
        "    avg_trace_tokens=('trace_tokens_approx', 'mean'),\n",
        "    accuracy=('correct', 'mean'),\n",
        ").round(2)\n",
        "\n",
        "summary['pct_has_trace'] = (summary['pct_has_trace'] * 100).round(1).astype(str) + '%'\n",
        "summary['accuracy'] = (summary['accuracy'] * 100).round(1).astype(str) + '%'\n",
        "\n",
        "print('=== Reasoning Model Summary ===')\n",
        "display(summary)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Response length distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Path('plots').mkdir(exist_ok=True)\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 4a. Total response length (all models)\n",
        "ax = axes[0]\n",
        "for model in ALL_MODELS:\n",
        "    subset = df[df['model'] == model]\n",
        "    if len(subset) == 0:\n",
        "        continue\n",
        "    vals = subset['total_response_chars'].clip(upper=subset['total_response_chars'].quantile(0.99))\n",
        "    ax.hist(vals, bins=50, alpha=0.5, label=model, density=True)\n",
        "ax.set_xlabel('Total response length (chars)')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title('Total Response Length Distribution')\n",
        "ax.legend(fontsize=8)\n",
        "\n",
        "# 4b. Reasoning trace length (reasoning models only, where trace exists)\n",
        "ax = axes[1]\n",
        "trace_df = reasoning_df[reasoning_df['has_any_trace']]\n",
        "for model in REASONING_MODELS:\n",
        "    subset = trace_df[trace_df['model'] == model]\n",
        "    if len(subset) == 0:\n",
        "        continue\n",
        "    vals = subset['trace_chars'].clip(upper=subset['trace_chars'].quantile(0.99))\n",
        "    ax.hist(vals, bins=50, alpha=0.5, label=model, density=True)\n",
        "ax.set_xlabel('Reasoning trace length (chars)')\n",
        "ax.set_ylabel('Density')\n",
        "ax.set_title('Reasoning Trace Length (where present)')\n",
        "ax.legend(fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('plots/reasoning_trace_length_dist.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Trace length by model × condition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "plot_data = reasoning_df.groupby(['model', 'condition'])['total_response_chars'].mean().reset_index()\n",
        "pivot = plot_data.pivot(index='model', columns='condition', values='total_response_chars')\n",
        "pivot = pivot.reindex(REASONING_MODELS)\n",
        "\n",
        "pivot.plot(kind='bar', ax=ax, width=0.7)\n",
        "ax.set_ylabel('Mean total response length (chars)')\n",
        "ax.set_title('Mean Response Length by Model and Condition')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n",
        "ax.legend(title='Condition')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('plots/reasoning_length_by_model_condition.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Trace length by model × domain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Heatmap: mean total response length — model vs domain\n",
        "trace_heatmap = reasoning_df.groupby(['model', 'domain'])['total_response_chars'].mean().reset_index()\n",
        "pivot_heat = trace_heatmap.pivot(index='model', columns='domain', values='total_response_chars')\n",
        "pivot_heat = pivot_heat.reindex(REASONING_MODELS)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(18, 5))\n",
        "sns.heatmap(pivot_heat, annot=True, fmt='.0f', cmap='YlOrRd', ax=ax,\n",
        "            linewidths=0.5, cbar_kws={'label': 'Mean response chars'})\n",
        "ax.set_title('Mean Response Length (chars) — Model × Domain')\n",
        "ax.set_xlabel('')\n",
        "ax.set_ylabel('')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('plots/reasoning_heatmap_model_domain.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Does longer reasoning → better accuracy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, len(REASONING_MODELS), figsize=(4 * len(REASONING_MODELS), 5),\n",
        "                         sharey=True)\n",
        "\n",
        "for i, model in enumerate(REASONING_MODELS):\n",
        "    ax = axes[i]\n",
        "    subset = reasoning_df[reasoning_df['model'] == model].copy()\n",
        "    if len(subset) == 0:\n",
        "        ax.set_title(f'{model}\\n(no data)')\n",
        "        continue\n",
        "\n",
        "    # Bin response length into quintiles\n",
        "    subset['length_bin'] = pd.qcut(\n",
        "        subset['total_response_chars'], q=5, labels=False, duplicates='drop'\n",
        "    )\n",
        "    bin_acc = subset.groupby('length_bin')['correct'].mean()\n",
        "    bin_labels = subset.groupby('length_bin')['total_response_chars'].mean()\n",
        "\n",
        "    ax.bar(range(len(bin_acc)), bin_acc.values, color=sns.color_palette('viridis', 5))\n",
        "    ax.set_xticks(range(len(bin_acc)))\n",
        "    ax.set_xticklabels([f'{v:.0f}' for v in bin_labels.values], rotation=45, fontsize=8)\n",
        "    ax.set_title(model, fontsize=10)\n",
        "    ax.set_xlabel('Avg chars in bin')\n",
        "    if i == 0:\n",
        "        ax.set_ylabel('Accuracy')\n",
        "    ax.set_ylim(0, 1.05)\n",
        "\n",
        "fig.suptitle('Accuracy by Response Length Quintile', fontsize=13, y=1.02)\n",
        "plt.tight_layout()\n",
        "plt.savefig('plots/accuracy_vs_reasoning_length.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Reasoning vs non-reasoning: accuracy comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "acc_by_model = df.groupby(['model', 'condition']).agg(\n",
        "    accuracy=('correct', 'mean'),\n",
        "    n=('correct', 'count'),\n",
        "    avg_response_len=('total_response_chars', 'mean'),\n",
        ").round(4).reset_index()\n",
        "\n",
        "acc_by_model['is_reasoning'] = acc_by_model['model'].isin(REASONING_MODELS)\n",
        "acc_by_model = acc_by_model.sort_values(['condition', 'is_reasoning', 'model'])\n",
        "\n",
        "print('=== Accuracy & Response Length by Model and Condition ===')\n",
        "display(acc_by_model.style.format({\n",
        "    'accuracy': '{:.1%}',\n",
        "    'avg_response_len': '{:,.0f}',\n",
        "}).background_gradient(subset=['accuracy'], cmap='RdYlGn', vmin=0, vmax=1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Side-by-side bar chart: reasoning vs non-reasoning accuracy by condition\n",
        "fig, ax = plt.subplots(figsize=(14, 6))\n",
        "\n",
        "order = NON_REASONING_MODELS + REASONING_MODELS\n",
        "\n",
        "sns.barplot(data=acc_by_model, x='model', y='accuracy', hue='condition',\n",
        "            order=order, ax=ax)\n",
        "\n",
        "# Add vertical separator between reasoning and non-reasoning\n",
        "ax.axvline(x=len(NON_REASONING_MODELS) - 0.5, color='gray', linestyle='--', alpha=0.5)\n",
        "ax.text(1, 1.02, 'Non-reasoning', transform=ax.get_xaxis_transform(),\n",
        "        ha='center', fontsize=10, color='gray')\n",
        "ax.text(len(NON_REASONING_MODELS) + len(REASONING_MODELS) / 2, 1.02, 'Reasoning',\n",
        "        transform=ax.get_xaxis_transform(), ha='center', fontsize=10, color='gray')\n",
        "\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Model Accuracy: Reasoning vs Non-Reasoning')\n",
        "ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n",
        "ax.set_ylim(0, 1.05)\n",
        "ax.legend(title='Condition')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('plots/accuracy_reasoning_vs_nonreasoning.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Detailed domain-level breakdown (reasoning models)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "domain_stats = reasoning_df.groupby(['model', 'domain', 'condition']).agg(\n",
        "    n=('raw_response', 'count'),\n",
        "    accuracy=('correct', 'mean'),\n",
        "    avg_total_chars=('total_response_chars', 'mean'),\n",
        "    avg_trace_chars=('trace_chars', 'mean'),\n",
        "    pct_has_trace=('has_any_trace', 'mean'),\n",
        ").round(3).reset_index()\n",
        "\n",
        "domain_stats['accuracy'] = (domain_stats['accuracy'] * 100).round(1)\n",
        "domain_stats['pct_has_trace'] = (domain_stats['pct_has_trace'] * 100).round(1)\n",
        "\n",
        "print('=== Domain-Level Reasoning Stats ===')\n",
        "display(domain_stats.sort_values(['model', 'condition', 'domain']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Difficulty vs reasoning effort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Do models reason more on harder problems?\n",
        "if 'difficulty' in reasoning_df.columns and reasoning_df['difficulty'].notna().any():\n",
        "    diff_data = reasoning_df[reasoning_df['has_any_trace']].copy()\n",
        "\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "\n",
        "    # 10a. Trace length vs difficulty\n",
        "    ax = axes[0]\n",
        "    for model in REASONING_MODELS:\n",
        "        subset = diff_data[diff_data['model'] == model]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        grouped = subset.groupby('difficulty')['trace_chars'].mean()\n",
        "        ax.plot(grouped.index, grouped.values, marker='o', label=model)\n",
        "    ax.set_xlabel('Difficulty')\n",
        "    ax.set_ylabel('Mean trace length (chars)')\n",
        "    ax.set_title('Reasoning Effort vs Problem Difficulty')\n",
        "    ax.legend(fontsize=8)\n",
        "\n",
        "    # 10b. Accuracy vs difficulty by model type\n",
        "    ax = axes[1]\n",
        "    for label, group in [('Reasoning', REASONING_MODELS), ('Non-reasoning', NON_REASONING_MODELS)]:\n",
        "        subset = df[df['model'].isin(group)]\n",
        "        if len(subset) == 0:\n",
        "            continue\n",
        "        grouped = subset.groupby('difficulty')['correct'].mean()\n",
        "        ax.plot(grouped.index, grouped.values, marker='o', label=label, linewidth=2)\n",
        "    ax.set_xlabel('Difficulty')\n",
        "    ax.set_ylabel('Accuracy')\n",
        "    ax.set_title('Accuracy vs Difficulty: Reasoning vs Non-Reasoning')\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('plots/difficulty_vs_reasoning.png', dpi=150, bbox_inches='tight')\n",
        "    plt.show()\n",
        "else:\n",
        "    print('No difficulty column or all NaN — skipping this analysis.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 11. Export summary tables"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
